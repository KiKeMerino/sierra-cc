# Predicci칩n de la cobertura de nieve con un modelo NARX

## Primeros pasos

Lo primero ser치 instalar los dos entornos necesarios para la ejecucion del proyecto con los siguientes ficheros: *'environment-hdf.yml'* y *'tf210_gpu.yml'*
Entornos para que funcione el proyecto correctamente, tanto para heatmaps.py como para algunas funcionalidades de limpieza_datos.py es necesario tener activo el entorno *environment-hdf.yml*. Para el resto usaremos *tf210_gpu.yml* ya que usar치 la versi칩n 2.10 de TensorFlow (libreria para machine learning) y har치 uso de la gpu (si el pc est치 configurado para ello) para procesar los datos m치s rapidamente. 

Para instalar el entorno simplemente habr치 que ejecutar *conda env create -f tf210_gpu.yml*
*conda env list* para comprobar que el entorno se ha creado correctamente y *conda activate tf210_gpu* para activar nuestro nuevo entorno


## 1. Obtenci칩n de Datos <a name="id1"> </a>

Los datos MODIS se obtuvieron de [EarthData Search](https://search.earthdata.nasa.gov/search), la plataforma de NASA para la b칰squeda de datos geoespaciales. Los pasos para la descarga personalizada fueron los siguientes:

1.  **Filtrado por Fecha y 츼rea de Inter칠s:**
    * Se filtraron los datos por el rango de fechas deseado y se defini칩 el 치rea de inter칠s correspondiente a la basin Adda-Bormio mediante un archivo .zip que contiene el .shp para delimitar el per칤metro.

2.  **Descarga Personalizada:**
    * Se seleccion칩 la opci칩n de descarga personalizada para tener control sobre el formato y la proyecci칩n de los datos.
        * ![Primera opci칩n](images/option1.png)
3.  **Reproyecci칩n geoespacial (Latitud/Longitud):**
    * Se solicit칩 que los datos fueran re proyectados al sistema de coordenadas geogr치ficas (latitud/longitud). Por defecto, los datos MODIS se proporcionan en proyecci칩n sinusoidal, que no es adecuada para muchos an치lisis comunes.
        * ![Segunda opci칩n](images/option2.png)
    * Es importante reproyectar los datos antes de la descarga para simplificar el procesamiento posterior.

    ## 游 Tipo de Modelo: NARX (Nonlinear AutoRegressive with eXogenous inputs)

Nuestro modelo se basa en la arquitectura **NARX (Nonlinear AutoRegressive with eXogenous inputs)**, que se implementa mediante **Redes Neuronales Recurrentes (RNN)** con **capas LSTM (Long Short-Term Memory)**.

## 2. Distribuci칩n de ficheros
El proyecto se divide en 2 grandes partes: el disco externo y el directorio en el que se encuentra este mismo README

Dentro del disco externo habr치 dos directorios:
- **data:** en el que se encontrar치n todos los CSVs y los archivos hdf descargados
    ->
- **models:** en el que habr치 una carpeta por cada cuenca que contendr치 el mejor modelo para esa cuenca asi como sus m칠tricas, y gr치ficas relevantes con respecto a las predicciones:
    - *future_predictions*: se mostrar치n las predicciones de los 5 modelos en los 4 escenarios posibles en cada cuenca 
    - *graphs_adda-bornio*: gr치ficas sobre el rendimiento del modelo comparado con los datos reales
    - *metrics.json*: metricas e hiperpar치metros del modelo
    - *narx_model_adda-bornio.h5*: el modelos

![Ejemplo de la carpeta del modelo de adda-bornio](images/ejemplo-ficheros.png)

## 3. Ficheros 칰tiles

### 5.1. limpieza_datos.py
Este fichero contiene un conjunto de funciones 칰tiles para procesar los datos y crear diferentes csv, a continuaci칩n se detallar치n brevemente las funciones contenidas

- **process_basin(basin):** funci칩n principal que calcula el area de la cuenca *'basin'*. Procesa cada uno de los archivos hdfs y calcula el area de nieve, el resultado se guarda en *EXTERNAL_DISK/data/csv/areas/*
- **process_var_exog(input_file, output_path, save=False):** coge el excel de series agregadas y lo convierte a csv separ치ndolo y renombrando las columnas. Devuelve un csv con todas las variables ex칩genas y con una nueva columna 'cuenca' que idenfica a qu칠 cuenca pertenece cada registro.
- **cleaning_future_series(input_data_path, output_data_path):** funci칩n que procesa el excel *EXTERNAL_DISK:\data\csv\Series_historicas-futuras.xlsx* y crea un csv con las varibles ex칩genas para cada escenario y cada modelo, en total saldr치n 20 csv distintos
- **join_area_exog(exog_file, areas_path, output_path = './datasets', save=False):** funci칩n para obtener el el dataset final de cada cuenca, coge como par치metro el csv de variables ex칩genas, el csv de areas calculado anteriormente y los junta, preparado para entrenar al modelo
- **impute_outliers(df, cuenca, columna, save=False):** funci칩n que coge un dataframe, y quita los outliers de la columna especificada por par치metro. Se considerar치 outlier cualquier valor por encima de *1.5 * rango_intercuartilico*

### 5.2. models/best_params.py
Programa muy 칰til que hace uso de la librer칤a optuna y se encarga de encontrar el mejor modelo para cada cuenca. Simplemente ejecutar el script y se pedir치 al usuario la cuenca que se desea optimizar y el n칰mero de ensayos que se quiere realizar. Cada ensayo tarda bastante por lo que se recomienda no usar un n칰mero demasiado alto, ej: 10-20.
Se guardar치n los hiperpar치metros del mejor modelo encontrado en un json, el cual se mostrar치 la ruta por pantalla
La m칠trica que se usa para la optimizaci칩n es el NSE (Nash Sutcliffe Efficiency)

### 5.3. models/create_load_model.py
Una vez se conocen la mejor configuraci칩n, para un modelo, este script crear치 o evaluar치 un nuevo modelo y se crear치n gr치ficas para mejor visualizaci칩n. Al igual que el fichero anterior, simplemente se ejecuta y el programa se encargar치 de pedir los datos al usuario

### 5.4. models/predictions.py
Este script pide al usuario el nombre de una cuenca, y el escenario del que se desea obtener la predicciones: obtiene los datasets de variables ex칩genas de cada modelo para ese escenario y genera una gr치fica en la que se visualizan las diferentes predicciones y un csv con las predcicciones de cada modelo.

### 5.5. heatmaps.py
Se encarga de generar los mapas de probabilidad de que cada pixel est칠 cubierno o no de nieve, leyendo los archivos hdf.
쮺omo usar?
Simplemente llamar a la funcion, save = True para guardar los resultados o False simplemente para mostrarlos por pantalla 

### 5.6. environment-hdf.yml & tf210_gpu.yml (IMPORTANTE)
Entornos para que funcione el proyecto correctamente, tanto para heatmaps.py como para algunas funcionalidades de limpieza_datos.py es necesario tener activo el entorno *environment-hdf.yml*. Para el resto usaremos *tf210_gpu.yml* ya que usar치 la versi칩n 2.10 de TensorFlow (libreria para machine learning) y har치 uso de la gpu (si el pc est치 configurado para ello) para procesar los datos m치s rapidamente. 

Para instalar el entorno simplemente habr치 que ejecutar *conda env create -f tf210_gpu.yml*
*conda env list* para comprobar que el entorno se ha creado correctamente y *conda activate tf210_gpu* para activar nuestro nuevo entorno